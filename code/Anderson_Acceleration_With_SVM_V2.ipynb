{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a555a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class LinearSVCWithAA(LinearSVC):\n",
    "    def __init__(self, learning_rate=1.0, alpha=0.1, n_iterations=10, verbose=False):        \n",
    "        # Initialize the attributes of the LinearSVCWithAA class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.n_iterations = n_iterations\n",
    "        self.verbose = verbose\n",
    "        self.loss = []  # Add this line to store the loss values\n",
    "\n",
    "#         # Initialize the coef_ attribute\n",
    "# #         self.coef_ = None\n",
    "#         self.coef_ = np.random.randn(5,)\n",
    "    \n",
    "#         # Initialize the coef_prev attribute\n",
    "#         self.coef_prev = self.coef_\n",
    "      \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, n_iterations=100, alpha=0.1,learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels.\n",
    "        n_iterations : int\n",
    "            Number of iterations to run the algorithm.\n",
    "        alpha : float\n",
    "            Anderson acceleration factor.\n",
    "        \"\"\"\n",
    "        # Convert the labels to one-hot encoded labels\n",
    "        y_one_hot = np.eye(np.max(y) + 1)[y]\n",
    "\n",
    "        # Initialize the weight vector and the loss list\n",
    "#         self.coef_ = np.random.randn(X.shape[1])\n",
    "#         self.coef_ = np.random.randn(X.shape[1], len(np.unique(y)))\n",
    "        self.coef_ = np.random.randn(X.shape[1], X.shape[1])\n",
    "#         loss = []\n",
    "\n",
    "        # Initialize the weight vector history\n",
    "        coef_history = [self.coef_]\n",
    "\n",
    "        # Run the Perceptron algorithm with Anderson acceleration\n",
    "        for i in range(n_iterations):\n",
    "            # Compute the gradient\n",
    "            grad = self._gradient(X, y)\n",
    "\n",
    "            # Add the updated weight vector to the history\n",
    "            coef_history.append(self.coef_)\n",
    "\n",
    "            # If we have enough past weight vectors, perform Anderson acceleration\n",
    "            if len(coef_history) > 2:\n",
    "                # Compute the difference between the current weight vector and the previous one\n",
    "                diff = coef_history[-1] - coef_history[-2]\n",
    "\n",
    "                # Update the current weight vector using Anderson acceleration\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "                self.coef_ += alpha * diff + grad[0]\n",
    "            else:\n",
    "                # Update the current weight vector using the gradient\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "#                 print(\"grad \",grad[0])\n",
    "                self.coef_ += learning_rate * grad[0]\n",
    "\n",
    "            # Compute the loss for this iteration\n",
    "            y_pred = self.predict(X)\n",
    "#             iter_loss = self.hinge_loss(y,y_pred)\n",
    "#             iter_loss = np.maximum(0, 1 - y * y_pred)\n",
    "#             print(\"iter_loss: \",iter_loss)\n",
    "            #######################################################\n",
    "            # Compute the loss for this sample\n",
    "#             sample_loss = -np.sum(y * np.log(y_pred))\n",
    "            # Increment the loss and the number of correct predictions for this iteration\n",
    "#             iter_loss += int(sample_loss)\n",
    "#             n_correct += int(np.argmax(y_pred) == np.argmax(y))\n",
    "            # Compute the loss for this iteration\n",
    "#             loss_value = np.maximum(0, 1 - y * y_pred) # hinge loss\n",
    "            loss_value = self.cross_entropy_loss(y, y_pred)\n",
    "#             print(\"Iteration: \", i, \", Loss: \",loss_value)\n",
    "            self.loss.append(loss_value)\n",
    "#             self.loss.append(iter_loss)\n",
    "            #########################################################\n",
    "\n",
    "            # Save the previous weight vector\n",
    "            self.coef_prev = self.coef_\n",
    "\n",
    "        # Save the number of iterations and the alpha value\n",
    "        self.n_iterations_ = n_iterations\n",
    "        self.alpha_ = alpha\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_pred):\n",
    "        # Compute the loss for each sample\n",
    "#         print(\"y length: \",len(y))\n",
    "#         print(\"y_pred length: \",len(y_pred))\n",
    "        sample_loss = -np.sum(y * np.log(y_pred + 1e-10))\n",
    "\n",
    "        # Average the loss over the number of samples\n",
    "        loss = np.mean(sample_loss)\n",
    "#         print(\"sample_loss\",sample_loss,\", loss\",loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def hinge_loss(self, X, y):\n",
    "        # Compute the prediction for each sample\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Compute the hinge loss for each sample\n",
    "        loss = np.maximum(0, 1 - y * y_pred)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "#         # Compute the class scores for each sample\n",
    "#         scores = np.dot(X, self.coef_.T)\n",
    "\n",
    "#         # Predict the class for each sample\n",
    "#         y_pred = np.argmax(scores)\n",
    "\n",
    "#         return y_pred\n",
    "    \n",
    "        # Predict the class of each sample in X\n",
    "        y_pred = np.dot(X, self.coef_)\n",
    "\n",
    "        # Normalize the predictions\n",
    "        y_pred = y_pred / np.sum(y_pred, axis=1, keepdims=True)\n",
    "\n",
    "        # Return the predicted class for each sample\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    def _gradient(self, X, y):\n",
    "        # Compute the prediction for each sample\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Compute the gradient of the loss function with respect to the weight vector\n",
    "        \n",
    "        grad_coef = np.dot(X.T, np.maximum(0, 1 - y * y_pred))\n",
    "        \n",
    "        # Average the gradient over the number of samples\n",
    "#         grad_coef /= len(X)\n",
    "        grad_coef = np.divide(grad_coef, len(X))\n",
    "        \n",
    "        grad_coef = np.squeeze(grad_coef) ###################################################\n",
    "\n",
    "        # Compute the gradient of the loss function with respect to the bias term\n",
    "        grad_intercept = np.sum(np.maximum(0, 1 - y * y_pred)) / len(X)\n",
    "\n",
    "        # Return the gradient as a tuple\n",
    "        return (grad_coef, grad_intercept)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3e51b8-54bf-44e0-9321-4eb2c38403b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457c11b8-6de9-4a99-be81-16b94761a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"C:/Users/pchourasia1/Desktop/Anderson/data/pca_embeddings/spike/kmer_Frequency_Vector_7000_PCA_500.npy\")\n",
    "attribute_data = np.load(\"C:/Users/pchourasia1/Desktop/Anderson/data/pca_embeddings/spike/seq_data_variant_names_7000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba6bcf7-2d92-4e2b-9333-015925308dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# items = []\n",
    "\n",
    "# with open('C:/Users/pchourasia1/Desktop/tSNE-Evaluation/first_variant_name_data_7000.csv') as csvfile:    \n",
    "# \tcsvReader = csv.reader(csvfile)    \n",
    "# \tfor row in csvReader:        \n",
    "# \t\titems.append(row[0])        \n",
    "# print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15d7e59-998e-4b5d-b8de-305e341d10d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute data preprocessing Done\n"
     ]
    }
   ],
   "source": [
    "attr_new = []\n",
    "for i in range(len(attribute_data)):\n",
    "    aa = str(attribute_data[i]).replace(\"[\",\"\")\n",
    "    aa_1 = aa.replace(\"]\",\"\")\n",
    "    aa_2 = aa_1.replace(\"\\'\",\"\")\n",
    "    attr_new.append(aa_2)\n",
    "\n",
    "unique_hst = list(np.unique(attr_new))\n",
    "\n",
    "int_hosts = []\n",
    "for ind_unique in range(len(attr_new)):\n",
    "    variant_tmp = attr_new[ind_unique]\n",
    "    ind_tmp = unique_hst.index(variant_tmp)\n",
    "    int_hosts.append(ind_tmp)\n",
    "    \n",
    "print(\"Attribute data preprocessing Done\")\n",
    "\n",
    "y = np.array(int_hosts[:])\n",
    "\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.3)\n",
    "\n",
    "sss.get_n_splits(X, y)\n",
    "train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.1,random_state=68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313446f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary libraries\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.special import softmax\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # # Split the data into training and test sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# n_classes = 2\n",
    "# n_features = 11\n",
    "\n",
    "# train_sample_size = 1000\n",
    "# # Load the training data and labels\n",
    "# X_train = np.random.randn(train_sample_size, n_features)\n",
    "# y_train = np.random.randint(n_classes, size=train_sample_size)\n",
    "\n",
    "# # Load the test data and labels\n",
    "# testing_sample_size = 300\n",
    "# X_test = np.random.randn(testing_sample_size, n_features)\n",
    "# y_test = np.random.randint(n_classes, size=testing_sample_size)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a93fa1c-cd61-4400-aa3f-da278f6251b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.004285714285714286\n",
      "Precision: 0.0010011123470522803\n",
      "Recall: 0.006206896551724138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pchourasia1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pchourasia1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the LinearSVCWithAA class\n",
    "model = LinearSVCWithAA()\n",
    "model.fit(X_train, y_train,n_iterations=10, learning_rate=0.1)\n",
    "\n",
    "# Predict the classes of the test set\n",
    "# y_pred = []\n",
    "# for i in range(len(X_test)):\n",
    "#     y_pred.append(model.predict(X_test[i]))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the classification accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Compute the precision\n",
    "precision = precisio\n",
    "n_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Compute the recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9263bf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a LinearSVCWithAA object with Anderson acceleration\n",
    "learning_rate_val = 0.05\n",
    "n_iterations_value = 2\n",
    "svc_aa = LinearSVCWithAA(alpha=0.5, learning_rate=learning_rate_val)\n",
    "\n",
    "# Create a LinearSVCWithAA object without Anderson acceleration\n",
    "svc_no_aa = LinearSVCWithAA(alpha=0, learning_rate=learning_rate_val)\n",
    "\n",
    "# Fit the models on the training data\n",
    "svc_aa.fit(X_train, y_train,n_iterations=n_iterations_value, learning_rate=learning_rate_val)\n",
    "svc_no_aa.fit(X_train, y_train,n_iterations=n_iterations_value, learning_rate=learning_rate_val)\n",
    "\n",
    "# Plot the loss for the two models\n",
    "plt.plot(svc_aa.loss, label='With Anderson acceleration')\n",
    "plt.plot(svc_no_aa.loss, label='Without Anderson acceleration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.savefig(\"E:/RA/Anderson_Acceleration/Plots/SVM.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5d62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "addf4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_aa.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038501a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
