{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776743ac",
   "metadata": {},
   "source": [
    "# With Gradient Descent Instead of Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f00b4d-fd7b-44e4-ac1d-e1413f83c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "\n",
    "def softmax(z, epsilon=1e-10):\n",
    "    # Subtract the maximum value from the logits\n",
    "    z = z - np.max(z, keepdims=True)\n",
    "    \n",
    "    # Compute the exponentiated logits\n",
    "    exp_z = np.exp(z)\n",
    "    \n",
    "    # Normalize the exponentiated logits to obtain the class probabilities\n",
    "    return exp_z / (np.sum(exp_z, keepdims=True) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00acdab5-96b9-455b-b34b-acce6b039e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"C:/Users/pchourasia1/Desktop/Anderson/data/pca_embeddings/spike/kmer_Frequency_Vector_7000_PCA_500.npy\")\n",
    "attribute_data = np.load(\"C:/Users/pchourasia1/Desktop/Anderson/data/pca_embeddings/spike/seq_data_variant_names_7000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f259ee-d26a-4571-805d-ce88c952fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute data preprocessing Done\n"
     ]
    }
   ],
   "source": [
    "attr_new = []\n",
    "for i in range(len(attribute_data)):\n",
    "    aa = str(attribute_data[i]).replace(\"[\",\"\")\n",
    "    aa_1 = aa.replace(\"]\",\"\")\n",
    "    aa_2 = aa_1.replace(\"\\'\",\"\")\n",
    "    attr_new.append(aa_2)\n",
    "\n",
    "unique_hst = list(np.unique(attr_new))\n",
    "\n",
    "int_hosts = []\n",
    "for ind_unique in range(len(attr_new)):\n",
    "    variant_tmp = attr_new[ind_unique]\n",
    "    ind_tmp = unique_hst.index(variant_tmp)\n",
    "    int_hosts.append(ind_tmp)\n",
    "    \n",
    "print(\"Attribute data preprocessing Done\")\n",
    "\n",
    "y = np.array(int_hosts[:])\n",
    "\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.3)\n",
    "\n",
    "sss.get_n_splits(X, y)\n",
    "train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.1,random_state=68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f57cf9-f7ac-4209-a433-ce3e1739166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def perceptron_aa(X, y, n_iterations, alpha):\n",
    "# n_classes = 22\n",
    "# n_features = 100\n",
    "# # n_iterations = 10\n",
    "\n",
    "# train_sample_size = 10000\n",
    "# # Load the training data and labels\n",
    "# X_train = np.random.randn(train_sample_size, n_features)\n",
    "# y_train = np.random.randint(n_classes, size=train_sample_size)\n",
    "\n",
    "# # Load the test data and labels\n",
    "# testing_sample_size = 3000\n",
    "# X_test = np.random.randn(testing_sample_size, n_features)\n",
    "# y_test = np.random.randint(n_classes, size=testing_sample_size)\n",
    "\n",
    "\n",
    "# total_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b80ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Alpha Value: 0.1, test accuracy:  0.028095238095238097\n",
      "For Alpha Value: 0.2, test accuracy:  0.0\n",
      "For Alpha Value: 0.3, test accuracy:  0.0\n",
      "For Alpha Value: 0.4, test accuracy:  0.0\n",
      "For Alpha Value: 0.5, test accuracy:  0.3871428571428571\n"
     ]
    }
   ],
   "source": [
    "# Set the number of iterations\n",
    "n_iterations = 10\n",
    "\n",
    "# Run the Perceptron algorithm with Anderson acceleration using different values of alpha\n",
    "alpha_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# alpha_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "losses = []\n",
    "for alpha in alpha_values:\n",
    "    #w, loss, acc = perceptron_aa_2(X_train, y_train, n_iterations, alpha)\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    # Convert the labels to one-hot encoded labels\n",
    "    y_one_hot = np.eye(np.max(y_train) + 1)[y_train]\n",
    "    \n",
    "    # Initialize the weight vector, loss list, and accuracy list\n",
    "#     w = np.random.randn(X_train.shape[1])\n",
    "    w = np.random.randn(X_train.shape[1],X_train.shape[1])\n",
    "#     w = np.random.randn(n_features,n_features)\n",
    "#     w = np.random.randn(X_train.shape[1])\n",
    "    \n",
    "    loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    # Initialize the weight vector history\n",
    "    w_history = [w]\n",
    "    \n",
    "    # Run the Perceptron algorithm with Anderson acceleration\n",
    "    for i in range(n_iterations):\n",
    "        # Initialize the loss and the number of correct predictions for this iteration\n",
    "        iter_loss = 0\n",
    "        n_correct = 0\n",
    "        \n",
    "        # Initialize the gradient\n",
    "        grad = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        # Loop through the training data and compute the gradient\n",
    "#         for x, y in zip(X_train, y_one_hot):\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            # Predict the class using the current weight vector\n",
    "            y_pred = np.dot(w, x)\n",
    "            \n",
    "            # Normalize the prediction\n",
    "            y_pred = y_pred / np.sum(y_pred)\n",
    "            \n",
    "            y_pred=softmax(y_pred)\n",
    "            \n",
    "            # Compute the gradient for this sample\n",
    "            grad += y - y_pred\n",
    "            \n",
    "            # Compute the loss for this sample\n",
    "            sample_loss = -np.sum(y * np.log(y_pred + 0.0001))\n",
    "            \n",
    "            # Increment the loss and the number of correct predictions for this iteration\n",
    "            iter_loss += sample_loss\n",
    "            n_correct += int(np.argmax(y_pred) == np.argmax(y))\n",
    "        \n",
    "        # Average the gradient over the number of samples\n",
    "        grad /= len(X_train)\n",
    "        \n",
    "        # Add the updated weight vector to the history\n",
    "        w_history.append(w)\n",
    "        \n",
    "        # If we have enough past weight vectors, perform Anderson acceleration\n",
    "        if len(w_history) > 2:\n",
    "            # Compute the difference between the current weight vector and the previous one\n",
    "            diff = w_history[-1] - w_history[-2]\n",
    "            \n",
    "            # Update the current weight vector using Anderson acceleration\n",
    "            w += alpha * diff + grad\n",
    "        else:\n",
    "            # Update the current weight vector using the gradient\n",
    "            w += grad\n",
    "        \n",
    "        # Append the loss and the accuracy to their respective lists\n",
    "        loss.append(iter_loss / len(X_train))\n",
    "        accuracy.append(n_correct / len(X_train))\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    losses.append(loss)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = np.dot(X_test, w)\n",
    "\n",
    "    # Compute the accuracy on the test set\n",
    "    y_pred_all = []\n",
    "    for ii in range(len(y_pred)):\n",
    "        y_pred_all.append(np.argmax(y_pred[ii]))\n",
    "    test_accuracy = np.mean(y_pred_all == y_test)\n",
    "    print(\"For Alpha Value: \" + str(alpha) + \", test accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4c02c8-fb16-4800-a85b-e4c3cdda9153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss for each value of alpha\n",
    "for i, loss in enumerate(losses):\n",
    "    plt.plot(loss, label=f'alpha={alpha_values[i]:.1f}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2d37378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25975.230133</td>\n",
       "      <td>25422.206337</td>\n",
       "      <td>25358.450716</td>\n",
       "      <td>25352.563264</td>\n",
       "      <td>25402.171782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25345.917661</td>\n",
       "      <td>25345.917627</td>\n",
       "      <td>25345.916396</td>\n",
       "      <td>25345.916649</td>\n",
       "      <td>25345.916426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25345.916149</td>\n",
       "      <td>25345.916120</td>\n",
       "      <td>25345.916021</td>\n",
       "      <td>25345.916067</td>\n",
       "      <td>25345.916025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25345.915978</td>\n",
       "      <td>25345.915969</td>\n",
       "      <td>25345.915940</td>\n",
       "      <td>25345.915956</td>\n",
       "      <td>25345.915940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25345.915926</td>\n",
       "      <td>25345.915922</td>\n",
       "      <td>25345.915909</td>\n",
       "      <td>25345.915917</td>\n",
       "      <td>25345.915909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25345.915903</td>\n",
       "      <td>25345.915901</td>\n",
       "      <td>25345.915894</td>\n",
       "      <td>25345.915899</td>\n",
       "      <td>25345.915894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25345.915891</td>\n",
       "      <td>25345.915890</td>\n",
       "      <td>25345.915885</td>\n",
       "      <td>25345.915889</td>\n",
       "      <td>25345.915886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25345.915884</td>\n",
       "      <td>25345.915883</td>\n",
       "      <td>25345.915880</td>\n",
       "      <td>25345.915882</td>\n",
       "      <td>25345.915880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25345.915879</td>\n",
       "      <td>25345.915879</td>\n",
       "      <td>25345.915877</td>\n",
       "      <td>25345.915879</td>\n",
       "      <td>25345.915877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25345.915876</td>\n",
       "      <td>25345.915876</td>\n",
       "      <td>25345.915875</td>\n",
       "      <td>25345.915876</td>\n",
       "      <td>25345.915875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4\n",
       "0  25975.230133  25422.206337  25358.450716  25352.563264  25402.171782\n",
       "1  25345.917661  25345.917627  25345.916396  25345.916649  25345.916426\n",
       "2  25345.916149  25345.916120  25345.916021  25345.916067  25345.916025\n",
       "3  25345.915978  25345.915969  25345.915940  25345.915956  25345.915940\n",
       "4  25345.915926  25345.915922  25345.915909  25345.915917  25345.915909\n",
       "5  25345.915903  25345.915901  25345.915894  25345.915899  25345.915894\n",
       "6  25345.915891  25345.915890  25345.915885  25345.915889  25345.915886\n",
       "7  25345.915884  25345.915883  25345.915880  25345.915882  25345.915880\n",
       "8  25345.915879  25345.915879  25345.915877  25345.915879  25345.915877\n",
       "9  25345.915876  25345.915876  25345.915875  25345.915876  25345.915875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(losses).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1437885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0563753b",
   "metadata": {},
   "source": [
    "# SVM With Anderson Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cf5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class LinearSVCWithAA(LinearSVC):\n",
    "    def __init__(self, alpha=0.1, n_iterations=10, verbose=False):        \n",
    "        # Initialize the attributes of the LinearSVCWithAA class\n",
    "        self.alpha = alpha\n",
    "        self.n_iterations = n_iterations\n",
    "        self.verbose = verbose\n",
    "        self.loss = []  # Add this line to store the loss values\n",
    "\n",
    "#         # Initialize the coef_ attribute\n",
    "# #         self.coef_ = None\n",
    "#         self.coef_ = np.random.randn(5,)\n",
    "    \n",
    "#         # Initialize the coef_prev attribute\n",
    "#         self.coef_prev = self.coef_\n",
    "    \n",
    "    \n",
    "    def fit_old(self, X, y, n_iterations=100, alpha=0.1):\n",
    "        # Initialize the weight vector and the bias term\n",
    "        self.coef_ = np.random.randn(X.shape[1])\n",
    "        self.intercept_ = np.random.randn()\n",
    "        \n",
    "        n_classes = len(np.unique(y))\n",
    "        n_features = len(X[0])\n",
    "        self.coef_ = np.random.randn(n_classes, n_features)\n",
    "        self.coef_prev = self.coef_\n",
    "        \n",
    "        # Run the gradient descent algorithm with Anderson acceleration\n",
    "        for i in range(n_iterations):\n",
    "            # Compute the gradient\n",
    "            grad = self._gradient(X, y)\n",
    "            \n",
    "            # Update the weight vector and the bias term using Anderson acceleration\n",
    "#             grad[:-1] = np.expand_dims(grad[:-1], axis=0)\n",
    "#             grad[:-1] = grad[:-1][:, np.newaxis]\n",
    "#             self.coef_ += alpha * (self.coef_ - self.coef_prev) + grad[:-1]\n",
    "#             self.intercept_ += alpha * (self.intercept_ - self.intercept_prev) + grad[-1]\n",
    "            \n",
    "            # Save the current weight vector and bias term for the next iteration\n",
    "            self.coef_prev = self.coef_\n",
    "            self.intercept_prev = self.intercept_\n",
    "        \n",
    "        # Save the number of iterations and the alpha value\n",
    "        self.n_iterations_ = n_iterations\n",
    "        self.alpha_ = alpha\n",
    "      \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, n_iterations=100, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels.\n",
    "        n_iterations : int\n",
    "            Number of iterations to run the algorithm.\n",
    "        alpha : float\n",
    "            Anderson acceleration factor.\n",
    "        \"\"\"\n",
    "        # Convert the labels to one-hot encoded labels\n",
    "        y_one_hot = np.eye(np.max(y) + 1)[y]\n",
    "\n",
    "        # Initialize the weight vector and the loss list\n",
    "#         self.coef_ = np.random.randn(X.shape[1])\n",
    "#         self.coef_ = np.random.randn(X.shape[1], len(np.unique(y)))\n",
    "        self.coef_ = np.random.randn(X.shape[1], X.shape[1])\n",
    "#         loss = []\n",
    "\n",
    "        # Initialize the weight vector history\n",
    "        coef_history = [self.coef_]\n",
    "\n",
    "        # Run the Perceptron algorithm with Anderson acceleration\n",
    "        for i in range(n_iterations):\n",
    "            # Compute the gradient\n",
    "            grad = self._gradient(X, y)\n",
    "\n",
    "            # Add the updated weight vector to the history\n",
    "            coef_history.append(self.coef_)\n",
    "\n",
    "            # If we have enough past weight vectors, perform Anderson acceleration\n",
    "            if len(coef_history) > 2:\n",
    "                # Compute the difference between the current weight vector and the previous one\n",
    "                diff = coef_history[-1] - coef_history[-2]\n",
    "\n",
    "                # Update the current weight vector using Anderson acceleration\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "                self.coef_ += alpha * diff + grad[0]\n",
    "            else:\n",
    "                # Update the current weight vector using the gradient\n",
    "#                 print(\"self.coef_ \",self.coef_)\n",
    "#                 print(\"grad \",grad[0])\n",
    "                self.coef_ += grad[0]\n",
    "\n",
    "            # Compute the loss for this iteration\n",
    "            y_pred = self.predict(X)\n",
    "#             iter_loss = self.hinge_loss(y,y_pred)\n",
    "            iter_loss = np.maximum(0, 1 - y * y_pred)\n",
    "#             print(\"iter_loss: \",iter_loss)\n",
    "            #######################################################\n",
    "            # Compute the loss for this sample\n",
    "            sample_loss = -np.sum(y * np.log(y_pred))\n",
    "            # Increment the loss and the number of correct predictions for this iteration\n",
    "            iter_loss += int(sample_loss)\n",
    "#             n_correct += int(np.argmax(y_pred) == np.argmax(y))\n",
    "            # Compute the loss for this iteration\n",
    "            self.loss.append(np.maximum(0, 1 - y * y_pred))\n",
    "#             self.loss.append(iter_loss)\n",
    "            #########################################################\n",
    "\n",
    "            # Save the previous weight vector\n",
    "            self.coef_prev = self.coef_\n",
    "\n",
    "        # Save the number of iterations and the alpha value\n",
    "        self.n_iterations_ = n_iterations\n",
    "        self.alpha_ = alpha\n",
    "\n",
    "    def hinge_loss(self, X, y):\n",
    "        # Compute the prediction for each sample\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Compute the hinge loss for each sample\n",
    "        loss = np.maximum(0, 1 - y * y_pred)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict_old(self, X):    \n",
    "        # Compute the class prediction for each sample using the current weight vector\n",
    "        y_pred = np.dot(X, self.coef_)\n",
    "\n",
    "        # Normalize the prediction\n",
    "        y_pred = y_pred / np.sum(y_pred, keepdims=True)\n",
    "\n",
    "        # Return the class with the highest probability\n",
    "        return np.argmax(y_pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Compute the class scores for each sample\n",
    "        scores = np.dot(X, self.coef_.T)\n",
    "\n",
    "        # Predict the class for each sample\n",
    "        y_pred = np.argmax(scores)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         y_pred = np.dot(X, self.coef_)\n",
    "#         return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    \n",
    "    def _gradient(self, X, y):\n",
    "        # Compute the prediction for each sample\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Compute the gradient of the loss function with respect to the weight vector\n",
    "#         print(\"y shape:\",y.shape)\n",
    "#         print(\"y_pred shape:\",y_pred.shape)\n",
    "        \n",
    "#         y = np.expand_dims(y, axis=1)\n",
    "#         print(\"y again shape:\",y.shape)\n",
    "        \n",
    "        grad_coef = np.dot(X.T, np.maximum(0, 1 - y * y_pred))\n",
    "        \n",
    "        # Average the gradient over the number of samples\n",
    "        grad_coef /= len(X)\n",
    "        \n",
    "        grad_coef = np.squeeze(grad_coef) ###################################################\n",
    "\n",
    "        # Compute the gradient of the loss function with respect to the bias term\n",
    "        grad_intercept = np.sum(np.maximum(0, 1 - y * y_pred)) / len(X)\n",
    "\n",
    "        # Return the gradient as a tuple\n",
    "        return (grad_coef, grad_intercept)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826e32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.18\n",
      "Precision: 0.1653268209640759\n",
      "Recall: 0.15429067327638987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pchourasia1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "n_classes = 5\n",
    "n_features = 6\n",
    "\n",
    "train_sample_size = 1000\n",
    "# Load the training data and labels\n",
    "X_train = np.random.randn(train_sample_size, n_features)\n",
    "y_train = np.random.randint(n_classes, size=train_sample_size)\n",
    "\n",
    "# Load the test data and labels\n",
    "testing_sample_size = 300\n",
    "X_test = np.random.randn(testing_sample_size, n_features)\n",
    "y_test = np.random.randint(n_classes, size=testing_sample_size)\n",
    "#############################################################################\n",
    "\n",
    "# Train the model using the LinearSVCWithAA class\n",
    "model = LinearSVCWithAA()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes of the test set\n",
    "y_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    y_pred.append(model.predict(X_test[i]))\n",
    "\n",
    "\n",
    "\n",
    "# Compute the classification accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Compute the precision\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Compute the recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d4c1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearSVCWithAA object with Anderson acceleration\n",
    "svc_aa = LinearSVCWithAA(alpha=0.5)\n",
    "\n",
    "# Create a LinearSVCWithAA object without Anderson acceleration\n",
    "svc_no_aa = LinearSVCWithAA(alpha=0)\n",
    "\n",
    "# Fit the models on the training data\n",
    "svc_aa.fit(X_train, y_train)\n",
    "svc_no_aa.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4807f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for the two models\n",
    "plt.plot(svc_aa.loss, label='With Anderson acceleration')\n",
    "plt.plot(svc_no_aa.loss, label='Without Anderson acceleration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f1b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_loss_val = svc_aa.loss[0]\n",
    "aa_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a2175-5e71-490e-b85f-196fe4ec8f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
